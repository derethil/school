{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f12083d9ad0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.io import loadmat\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "torch.manual_seed(298742)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1  1 -1 ... -1  1  1]\n"
     ]
    }
   ],
   "source": [
    "#Load Mini MNIST from the MATLAB data file - Observe how we load the files\n",
    "\n",
    "mnist = loadmat('mnist_49_3000.mat')\n",
    "X = mnist['x']\n",
    "y = mnist['y'][0]\n",
    "\n",
    "print(y)\n",
    "'''\n",
    "Test Train Split for Calculating accuracy on a Held Out Test Dataset.\n",
    "'''\n",
    "X_train, X_test, y_train, y_test = train_test_split(X.T, y, test_size=1000, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We need to change -1 to 0 before fitting logistic regression.\n",
    "\n",
    "def converter(inp):\n",
    "    if inp > 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "y_train_mod = np.array([converter(val) for val in y_train])\n",
    "y_test_mod  = np.array([converter(val) for val in y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_train_mod = torch.tensor(y_train_mod, dtype=torch.float32)\n",
    "y_test_mod = torch.tensor(y_test_mod, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression(torch.nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(LinearRegression, self).__init__()\n",
    "        self.linear = torch.nn.Linear(input_size, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.linear(x)\n",
    "        out = torch.sigmoid(out)\n",
    "        return out\n",
    "\n",
    "class LogisticRegression:\n",
    "    \n",
    "    def __init__(self, lr=0.01, lamb = 10, num_iter=1000, fit_intercept=True):\n",
    "        self.lr = lr\n",
    "        self.lamb = lamb\n",
    "        self.num_iter = num_iter\n",
    "        self.fit_intercept = fit_intercept\n",
    "\n",
    "        self.model = LinearRegression(input_size=784, num_classes=1)\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        print(y.shape)\n",
    "\n",
    "        criterion = torch.nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=self.lamb)\n",
    "\n",
    "        for epoch in range(self.num_iter):\n",
    "            outputs = self.model(X_train)\n",
    "            loss = criterion(outputs.T[0], y)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "    def predict(self, X, threshold=0.5):\n",
    "        # return self.predict_prob(X) >= threshold\n",
    "        score, _ = torch.max(self.model(X).data, 1)\n",
    "        \n",
    "        for i in range(len(score)):\n",
    "            if score[i] < 0.5:\n",
    "                score[i] = 0\n",
    "            else:\n",
    "                score[i] = 1\n",
    "        \n",
    "        return score\n",
    "\n",
    "    def posterior_probability(self, X):\n",
    "        weights = self.model.linear.state_dict()['weight']\n",
    "        return 1 / (1 + torch.exp(-weights[0] @ X.T))\n",
    "    \n",
    "\n",
    "    def confidence(self, X):\n",
    "        weights = self.model.linear.state_dict()['weight']  \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2000])\n",
      "Test Error (lamb = 0.001): 2.4%\n",
      "torch.Size([2000])\n",
      "Test Error (lamb = 0.01): 2.4%\n",
      "torch.Size([2000])\n",
      "Test Error (lamb = 0): 2.4%\n"
     ]
    }
   ],
   "source": [
    "# torch.set_printoptions(threshold=5000)\n",
    "# print(y_train_mod)\n",
    "\n",
    "def report_lambda(lamb):\n",
    "    model = LogisticRegression(lr=.001, lamb=lamb)\n",
    "    model.fit(X_train, y_train_mod)\n",
    "    prediction = model.predict(X_test)\n",
    "\n",
    "    num_correct = (prediction == y_test_mod).sum().item()\n",
    "    accuracy = num_correct / len(y_test_mod)\n",
    "    \n",
    "    print(f'Test Error (lamb = {lamb}): {round((1 - accuracy) * 100, 4)}%')\n",
    "\n",
    "\n",
    "report_lambda(0.001)\n",
    "report_lambda(0.01)\n",
    "report_lambda(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2000])\n"
     ]
    }
   ],
   "source": [
    "def normalized_confidence(index):\n",
    "    confidence = confidences[index].item()\n",
    "\n",
    "    if confidence > .5:\n",
    "        return 1 - confidence\n",
    "    else:\n",
    "        return confidence\n",
    "\n",
    "# Train the model\n",
    "\n",
    "model = LogisticRegression(lr=.001, lamb=0.001)\n",
    "model.fit(X_train, y_train_mod)\n",
    "\n",
    "# Get the confidence on the test set\n",
    "\n",
    "prediction = model.predict(X_test)\n",
    "confidences = model.posterior_probability(X_test)\n",
    "\n",
    "# Get the misclassified images and their confidences\n",
    "\n",
    "is_correct = (prediction == y_test_mod)\n",
    "misclassified = [\n",
    "    (normalized_confidence(i), X_test[i], y_test[i].item()) \n",
    "    for i in range(len(prediction)) if not is_correct[i]\n",
    "]\n",
    "\n",
    "misclassified = sorted(misclassified, key=lambda v: v[0])\n",
    "\n",
    "# Display the top 20 most confident misclassifications\n",
    "\n",
    "plt.figure(figsize=(12,12))\n",
    "plt.suptitle(\"Most Confident Misclassified Images\", fontsize=25)\n",
    "\n",
    "for i, (confidence, image, label) in enumerate(misclassified[:20]):\n",
    "    plt.subplot(5,4,i+1)\n",
    "    plt.imshow(image.reshape(28, 28), cmap=\"gray\")\n",
    "    plt.title(9 if label == 1 else 4, fontsize=18)\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
